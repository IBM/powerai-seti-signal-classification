{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://www.cognitiveclass.ai\"><img src = \"https://cognitiveclass.ai/wp-content/themes/bdu3.0/static/images/cc-logo.png\" align = left></a>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "--------------------\n",
    "# Search for Extra Terrestrial Intelligence (SETI)\n",
    "###  SETI Signal Classification on PowerAI with Single GPU\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Introduction\n",
    "In this Notebook, we will use the famous [SETI Dataset](https://github.com/setiQuest/ML4SETI/) to build a Convolutional Neural Networks capable to perform signals classification. CNN will say, with some associated error, what type of signal is the presented input.\n",
    "\n",
    "### Project overview:\n",
    "Each night, using the Allen Telescope Array (ATA) in northern California, the SETI Institute scans the sky at various radio frequencies, observing star systems with known exoplanets, searching for faint but persistent signals. The current signal detection system is programmed to search only for particular kinds of signals: narrow-band carrier waves. However, the detection system sometimes triggers on signals that are not narrow-band signals  (with unknown efficiency) and are also not explicitly-known radio frequency interference (RFI). There seems to be various categories of these kinds of events that have been observed in the past.\n",
    "\n",
    "Our goal is to classify these accurately in real-time. This may allow the signal detection system to make better observational decisions, increase the efficiency of the nightly scans, and allow for explicit detection of these other signal types.\n",
    "\n",
    "For more information refer to [SETI hackathon page](https://github.com/setiQuest/ML4SETI/).\n",
    "\n",
    "\n",
    "### Performance\n",
    "Convelutional Neural Network involves a lot of matrix and vector multiplications that can parallelized, so GPUs can overperform, because GPUs were designed to handle these kind of matrix operations in parallel!\n",
    "\n",
    "### Why GPU overperforms?\n",
    "A single core CPU takes a matrix operation in serial, one element at a time. But, a single GPU could have hundreds or thousands of cores, while a CPU typically has no more than a few cores.\n",
    "\n",
    "\n",
    "### How to use GPU with TensorFlow?\n",
    "It is important to notice that if both CPU and GPU are available on the machine that you are running the noebook, and if a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device. \n",
    "\n",
    "In our case, as we are running this notebook on [IBM PowerAI](http://cocl.us/SETI-NIMBIX-PowerAI), you hvae access to multi GPU, but lets use one of the GPUs in this notebook, for the sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/opt/DL/tensorflow/lib/python2.7/site-packages/\")\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### SET YOUR WORKING SPACE HERE! Use this folder to save intermediate results.\n",
    "dataset_name = 'SETI_ds_64x128'\n",
    "data_dir = \"tmp/SETI1_data/\"\n",
    "train_dir = 'tmp/SETI1_train/'\n",
    "log_dir = train_dir + '1GPU_4'\n",
    "#check point directory\n",
    "chk_directory = train_dir + '/save1/'\n",
    "checkpoint_path = chk_directory + 'model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the needed folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(data_dir) is False:\n",
    "    os.makedirs(data_dir)\n",
    "print data_dir\n",
    "print os.popen(\"ls -lrt \"+ data_dir).read()\n",
    "if os.path.exists(train_dir) is False:\n",
    "    os.makedirs(train_dir)\n",
    "print train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Import dataset reader\n",
    "The signals for this notebook, have been converted to spectogram images, and stored as 4 files.\n",
    "The following cell will load a python code that help us to decode the binary file, and read the SETI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!wget -q --output-document  SETI.zip  https://ibm.box.com/shared/static/jhqdhcblhua5dx2t7ixwm88okitjrl6l.zip\n",
    "!unzip -o SETI.zip\n",
    "import SETI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Download data\n",
    "__Notice:__ The dataset exist and shared on IBM box. Running the following cell, you can download the dataset and extract it if you have not run the previouse notebook, i.e. the data generator notebook.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "    DATA_URL =  'https://ibm.box.com/shared/static/qz33lcio9ip2j8qi2atxqs62gn3bnu2s.gz'\n",
    "    dest_directory = data_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        #print 'No zip file exist in ', filepath\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "        sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    extracted_dir_path = os.path.join(dest_directory, dataset_name)\n",
    "    if not os.path.exists(extracted_dir_path):\n",
    "        print 'Extracting to', extracted_dir_path\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Load data SETI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ds_directory = data_dir + dataset_name\n",
    "dataset = SETI.read_data_sets(ds_directory, one_hot=True, validation_size=0)\n",
    "dataset.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Understanding the imported data\n",
    "\n",
    "The imported data can be divided as follow:\n",
    "\n",
    "- Training (dataset.train) >>  Use the given dataset with inputs and related outputs for training of NN. In our case, if you give an image that you know that represents a \"class1\", this set will tell the neural network that we expect a \"class1\" as the output.  \n",
    "        - 694 signals (images)\n",
    "        - dataset.train.images for inputs\n",
    "        - dataset.train.labels for outputs\n",
    "  \n",
    "  \n",
    "- Test (mnist.test) >> the model does not have access to this informations prior to the test phase. It is used to evaluate the performance and accuracy of the model against \"real life situations\". No further optimization beyond this point.  \n",
    "        - 10,000 data points\n",
    "        - dataset.test.images for inputs\n",
    "        - dataset.test.labels for outputs\n",
    "        \n",
    "        \n",
    "#### Labels\n",
    "- Each image (spectrum of signal) in the dataset has been labeled from 1 to 4, representing:\n",
    "    - squiggle\n",
    "    - narrowband\n",
    "    - noise\n",
    "    - narrowbanddrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "decay_rate=0.96\n",
    "decay_steps=500\n",
    "learning_rate = 0.005\n",
    "training_epochs = 20\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "n_classes = 4 # number of possible classifications for the problem\n",
    "dropout = 0.60 # Dropout, probability to keep units\n",
    "\n",
    "height = 64 # height of the image in pixels \n",
    "width = 128 # width of the image in pixels \n",
    "n_input = width * height # number of pixels in one image \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Inputs\n",
    "\n",
    "It's a best practice to create placeholders before variable assignments when using TensorFlow. Here we'll create placeholders for inputs (\"X\") and outputs (\"Y_\").   \n",
    "\n",
    "__Placeholder 'X':__ represents the \"space\" allocated input or the images. \n",
    "       * Each input has 8192 pixels distributed by a 64 width x 128 height matrix   \n",
    "       * The 'shape' argument defines the tensor size by its dimensions.  \n",
    "       * 1st dimension = None. Indicates that the batch size, can be of any size.  \n",
    "       * 2nd dimension = 8192. Indicates the number of pixels on a single flattened spectogram image.  \n",
    "      \n",
    "__Placeholder 'Y':___ represents the final output or the labels.  \n",
    "       * 4 possible classes (0,1,2,3)  \n",
    "       * The 'shape' argument defines the tensor size by its dimensions.  \n",
    "       * 1st dimension = None. Indicates that the batch size, can be of any size.   \n",
    "       * 2nd dimension = 4. Indicates the number of targets/outcomes \n",
    "\n",
    "__dtype for both placeholders:__ if you not sure, use tf.float32. The limitation here is that the later presented softmax function only accepts float32 or float64 dtypes. For more dtypes, check TensorFlow's documentation <a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/framework.html#tensor-types\">here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "x  = tf.placeholder(tf.float32, shape=[None, n_input], name = 'x')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, n_classes], name = 'y_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The input image is a 64 pixels by 128 pixels, 1 channel (grayscale). In this case, the first dimension is the __batch number__ of the image, and can be of any size (so we set it to -1). The second and third dimensions are width and hight, and the last one is the image channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,height,width,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Convolutional neural networks (CNNs)\n",
    "\n",
    "Convolutional neural networks (CNNs) is a type of feed-forward neural network, consist of multiple layers of  neurons that have learnable weights and biases. Each neuron in a layer that receives some input, process it, and optionally follows it with a non-linearity. The network has multiple layers such as convolution, max pool, drop out and fully connected layers. In each layer, small neurons process portions of the input image. The outputs of these collections are then tiled so that their input regions overlap, to obtain a higher-resolution representation of the original image; and it is repeated for every such layer. The important point here is: CNNs are able to break the complex patterns down into a series of simpler patterns, through multiple layers.\n",
    "\n",
    "### CNN architecture\n",
    "Now we are going to use a Deep Neural Network. \n",
    "\n",
    "\n",
    "Architecture of our network is:\n",
    "    \n",
    "- [Input] >>>> (batch_size, 64, 128, 1)  \n",
    "- [Convolutional layer 1]  >>>> (batch_size, 64, 128, 32)\n",
    "- [ReLU 1]  >>>> (batch_size, 64, 128, 32)\n",
    "- [Max pooling 1] >>>> (batch_size, 32, 64, 32)\n",
    "- [Convolutional layer 2]  >>>> (batch_size, 32, 64, 64)\n",
    "- [ReLU 2]  >>>> (batch_size, 32, 64, 64)\n",
    "- [Max pooling 2]  >>>> (batch_size, 8, 16, 64) \n",
    "- [fully connected layer 1] >>>> (batch_size, 1024)\n",
    "- [ReLU]  >>>> (batch_size, 1024)\n",
    "- [Drop out]  >>>> (batch_size, 1024)\n",
    "- [Readout layer] >>>> (batch_size, 1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Convolutional Layer 1\n",
    "We define a kernle here. The Size of the filter/kernel is 5x5;  Input channels is 1 (greyscale);  and we need 32 different feature maps (here, 32 feature maps means 32 different filters are applied on each image. So, the output of convolution layer would be 64x128x32). In this step, we create a filter / kernel tensor of shape `[filter_height, filter_width, in_channels, out_channels]`\n",
    "\n",
    "- To creat convolutional layer, we use __tf.nn.conv2d__. It computes a 2-D convolution given 4-D input and filter tensors.\n",
    "- The convolutional layer, slides the \"kernel window\" across the input tensor. As the input tensor has 4 dimensions:  [batch, height, width, channels], then the convolution operates on a 2D window on the height and width dimensions. __strides__ determines how much the window shifts by in each of the dimensions. As the first and last dimensions are related to batch and channels, we set the stride to 1. But for second and third dimension, we coould set other values, e.g. [1, 2, 2, 1]\n",
    "- __max pooling__ is a form of non-linear down-sampling. It partitions the input image into a set of rectangles and, and then find the maximum value for that region. Lets use __tf.nn.max_pool__ function to perform max pooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer1') as scope:\n",
    "    W_conv1 = tf.get_variable( initializer= tf.truncated_normal([5, 5, 1, 32], stddev=0.1), name= 'w1')\n",
    "    b_conv1 = tf.get_variable(initializer=tf.constant(0.1, shape=[32]), name='b1') # need 32 biases for 32 outputs\n",
    "    convolve1 = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME', name='convolv1') + b_conv1\n",
    "    h_conv1 = tf.nn.relu(convolve1, name='relu1')\n",
    "    conv1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1') #max_pool_2x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Convolutional Layer 2\n",
    "We apply the convolution again in this layer. Lets look at the second layer kernel:  \n",
    "- Filter/kernel: 5x5x32\n",
    "- Input channels: 32 (from the 1st Conv layer, we had 32 feature maps) , so we use 64 output feature maps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer2') as scope:\n",
    "    W_conv2 = tf.get_variable(initializer=tf.truncated_normal([5, 5, 32, 64], stddev=0.1), name= 'w2')\n",
    "    b_conv2 = tf.get_variable(initializer=tf.constant(0.1, shape=[64]),  name= 'b2') #need 64 biases for 64 outputs\n",
    "    convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME', name='convolv2')+ b_conv2\n",
    "    h_conv2 = tf.nn.relu(convolve2, name='relu2')\n",
    "    conv2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 4, 4, 1], padding='SAME', name='pool2') #max_pool_2x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Fully Connected Layer\n",
    "\n",
    "You need a fully connected layer to use the Softmax and create the probabilities in the end. Fully connected layers take the high-level filtered images from previous layer, that is all the 64 matrics, and convert them to a flat array.\n",
    "\n",
    "So, the matrix (8x16)x64 will be converted to a matrix of size (1x1024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dim = conv2.get_shape().as_list()\n",
    "dims = dim[1]*dim[2]*dim[3]\n",
    "nodes1 = 1024\n",
    "with tf.variable_scope('FullyCon1') as scope:\n",
    "    prv_layer_matrix = tf.reshape(conv2, [-1, dims], name='rshaped_inp')\n",
    "    W_fc1 = tf.get_variable(initializer= tf.truncated_normal([dims, nodes1], stddev=0.1), name='W_FC1')\n",
    "    b_fc1 = tf.get_variable(initializer= tf.constant(0.1, shape=[nodes1]), name='b_FC1') # need 1024 biases for 1024 outputs\n",
    "    h_fcl1  = tf.matmul(prv_layer_matrix, W_fc1) + b_fc1\n",
    "    fc_layer1 = tf.nn.relu(h_fcl1, name='relu_FC1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Droupout 1\n",
    "It is a phase where the network \"forget\" some features. At each training step in a mini-batch, some units get switched off randomly so that it will not interact with the network. That is, it weights cannot be updated, nor affect the learning of the other network nodes.  This can be very useful for very large neural networks to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32, name= 'keep_prob')\n",
    "layer_drop1 = tf.nn.dropout(fc_layer1, keep_prob, name='dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Readout Layer\n",
    "In last layer, CNN takes the high-level filtered images and translate them into votes using softmax. __softmax__ allows us to interpret the outputs of __fcl4__ as probabilities. So, __y_conv__ is a tensor of probablities.\n",
    "Input channels os this layer is 1024 (neurons from the 3rd Layer) pixel, and output features are 4 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('ReadoutLayer') as scope:\n",
    "    W_fc = tf.get_variable(initializer = tf.truncated_normal([nodes1, n_classes], stddev=0.1), name ='W_FC2') #1024 neurons\n",
    "    b_fc = tf.get_variable(initializer =tf.constant(0.1, shape=[n_classes]), name = 'b_FC2') # 10 possibilities for classes [0,1,2,3]\n",
    "    fc = tf.matmul(layer_drop1, W_fc) + b_fc\n",
    "    y_CNN= tf.nn.softmax(fc, name = 'softmax_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Loss function\n",
    "We need to compare our output, layer4 tensor, with ground truth for all mini_batch. we can use __cross entropy__ to see how bad our CNN is working - to measure the error at a softmax layer. __softmax_cross_entropy_with_logits__ \n",
    "computes softmax cross entropy between logits and labels, and __reduce_mean__ computes the mean of all elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_CNN, labels=y_))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Training\n",
    "It is obvious that we want minimize the error of our network which is calculated by cross_entropy metric. To solve the problem, we have to compute gradients for the loss (which is minimizing the cross-entropy) and apply gradients to variables. It will be done by an optimizer: GradientDescent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a variable to track the global step.\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "# create learning_decay\n",
    "lr = tf.train.exponential_decay( learning_rate,\n",
    "                                 global_step,\n",
    "                                 decay_steps,\n",
    "                                 decay_rate, staircase=True )\n",
    "tf.summary.scalar('learning_rate', lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the optimizer to apply the gradients that minimize the loss\n",
    "# (and also increment the global step counter) as a single training step.\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    train_op = optimizer.minimize(cross_entropy, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Evaluation\n",
    "Do you want to know how many of the cases in a mini-batch has been classified correctly? lets count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_all'):\n",
    "    pred_lb = tf.argmax(y_CNN,1)\n",
    "    true_lb = tf.argmax(y_,1)\n",
    "    correct_prediction = tf.equal(pred_lb,true_lb )\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy for test images\n",
    "def evaluate():\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        num_iter = int(math.ceil(dataset.test.num_examples / batch_size))\n",
    "        true_count = 0  # Counts the number of correct predictions.\n",
    "        total_sample_count = num_iter * batch_size\n",
    "        step = 0\n",
    "        while step < num_iter:\n",
    "            x_batch, y_batch = dataset.test.next_batch(batch_size)\n",
    "            predictions = sess.run([correct_prediction], feed_dict={x: x_batch, y_: y_batch, keep_prob: 1.})\n",
    "            true_count += np.sum(predictions)\n",
    "            step += 1\n",
    "\n",
    "        precision = true_count*1.0 / total_sample_count\n",
    "        tf.summary.scalar('precision', precision)\n",
    "        return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "directory = os.path.dirname(chk_directory)\n",
    "try:\n",
    "    os.stat(directory)\n",
    "    ckpt = tf.train.get_checkpoint_state(chk_directory)\n",
    "    print ckpt\n",
    "except:\n",
    "    os.mkdir(directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>*You can run this cell if you REALLY have time to wait, or you are running it using PowerAI   </strong></font>\n",
    "<br>\n",
    "\n",
    "<b> What is PowerAI?  </b>\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/DX0108EN-PowerAI).\n",
    "\n",
    "<br>\n",
    "__Notice:__ If you are running this notebook on PowerAI, it will automatically run on a GPU, otherwise it will use a CPU. Also, you can change the number of epoches to gain a higher accuracy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_examples = dataset.train.num_examples\n",
    "total_batch = int(num_examples / batch_size)\n",
    "print 'Training dataset size:',num_examples, ' Signals'\n",
    "print 'Testing dataset size:',dataset.test.num_examples, ' Signals'\n",
    "print 'Total epochs:', training_epochs\n",
    "print 'Total steps:', training_epochs * total_batch\n",
    "print 'Batch size:', batch_size\n",
    "print 'Total batchs per epoch:',total_batch\n",
    "print str(datetime.datetime.now())\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "loss_values = []\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "sess.run(init)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    \n",
    "X_test = dataset.test.images\n",
    "y_test = dataset.test.labels\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "# load previously trained model if appilcable\n",
    "ckpt = tf.train.get_checkpoint_state(chk_directory)\n",
    "if ckpt:\n",
    "    print (\"loading model: \",ckpt.model_checkpoint_path)\n",
    "    #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "# Training cycle\n",
    "tr_start = time.time()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    avg_train_acc = 0.\n",
    "    total_batch = int(num_examples / batch_size)\n",
    "\n",
    "    # Loop over all batches\n",
    "    start = time.time()\n",
    "    for step in range(total_batch):\n",
    "        x_batch, y_batch = dataset.train.next_batch(batch_size ,shuffle=True)\n",
    "        b_start_time = time.time()\n",
    "        loss, acc = sess.run([cross_entropy, accuracy], feed_dict={x: x_batch,y_: y_batch,keep_prob: 1.})\n",
    "        sess.run(train_op,feed_dict={x: x_batch, y_: y_batch, keep_prob: dropout})\n",
    "        b_end_time = time.time() - b_start_time\n",
    "        avg_loss += loss / total_batch\n",
    "        avg_train_acc += acc / total_batch\n",
    "        \n",
    "        \n",
    "    # save model every x epochs\n",
    "    if epoch >= 0 and epoch % 50 == 0:\n",
    "        # Save model\n",
    "        #print (\"model saved to {}\".format(checkpoint_path))\n",
    "        saver.save(sess, checkpoint_path, global_step = epoch)\n",
    "        \n",
    "        \n",
    "    ## Display model every 1 epochs\n",
    "    if epoch >= 0 and epoch % display_step == 0:\n",
    "        end = time.time()\n",
    "        plr = sess.run(lr)\n",
    "        loss_values.append(avg_loss) \n",
    "        test_acc  = evaluate()\n",
    "        print(\"Epoch:\"+ '%04d' % (epoch+1) + \\\n",
    "            \", Ep_time=\" + \"{:.2f}\".format(end - start) + \\\n",
    "            \", lr=\" + \"{:.9f}\".format(plr) + \\\n",
    "            \", avg_cost=\" + \"{:.3f}\".format(avg_loss) + \\\n",
    "            \", Train_Acc=\" + \"{:.2f}\".format(avg_train_acc) + \\\n",
    "            \", Test_Acc=\" + \"{:.2f}\".format(test_acc) + \n",
    "            \", Batch (sec)=\" + \"{:.3f}\".format(b_end_time)  ) \n",
    "        \n",
    "    # Summarize model every x epochs\n",
    "    if epoch >= 0 and epoch % 1 == 0:   \n",
    "        summary= sess.run(merged,feed_dict={x: x_batch,y_: y_batch,keep_prob: 1.})\n",
    "        train_writer.add_summary(summary, epoch)\n",
    "print(\"Wall Time:\",\"{:.1f}\".format((time.time() - tr_start)/60.0, \"Min\"))        \n",
    "print(\"Optimization Finished!\")\n",
    "print (\"model saved to {}\".format(checkpoint_path))\n",
    "saver.save(sess, checkpoint_path, global_step = (epoch+1)*step)\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the labels of test set\n",
    "y_pred_lb = sess.run(tf.argmax(y_CNN,1), feed_dict={x: X_test, y_: y_test, keep_prob: 1.})\n",
    "y_pred = sess.run(y_CNN, feed_dict={x: X_test, y_: y_test, keep_prob: 1.})\n",
    "\n",
    "# lets save kernels\n",
    "kernels_l1 = sess.run(tf.reshape(tf.transpose(W_conv1, perm=[2, 3, 0, 1]),[32,-1]))\n",
    "kernels_l2 = sess.run(tf.reshape(tf.transpose(W_conv2, perm=[2, 3, 0, 1]),[32*64,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([np.mean(loss_values[i:i+5]) for i in range(len(loss_values))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Accuracy is depend on the number of epoch that you set in partametrs part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "y_ = np.argmax(y_test,1) # ground truth\n",
    "print metrics.classification_report(y_true= y_, y_pred= y_pred_lb)\n",
    "print metrics.confusion_matrix(y_true= y_, y_pred= y_pred_lb)\n",
    "print(\"Classification accuracy: %0.6f\" % metrics.accuracy_score(y_true= y_, y_pred= y_pred_lb) )\n",
    "print(\"Log Loss: %0.6f\" % metrics.log_loss(y_true= y_, y_pred= y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Viz\n",
    "Do you want to look at all the filters? lets use __utils__ to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!wget --output-document utils1.py http://deeplearning.net/tutorial/code/utils.py\n",
    "import utils1\n",
    "from utils1 import tile_raster_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here you can plot the 32 filters in the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "image = Image.fromarray(tile_raster_images(kernels_l1, img_shape=(5, 5) ,tile_shape=(4, 8), tile_spacing=(1, 1)))\n",
    "### Plot image\n",
    "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
    "imgplot = plt.imshow(image)\n",
    "imgplot.set_cmap('gray')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Also, you can plot and take a look at some filters form second convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "image = Image.fromarray(tile_raster_images(kernels_l2, img_shape=(5, 5) ,tile_shape=(4, 12), tile_spacing=(1, 1)))\n",
    "### Plot image\n",
    "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
    "imgplot = plt.imshow(image)\n",
    "imgplot.set_cmap('gray')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To underestand better, lets apply one os these filters on a sample signal (spectogram image). First, lets plot a sample spectogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "sampleimage1 = X_test[3]\n",
    "plt.imshow(np.reshape(sampleimage1,[64,128]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we apply different filters on them, and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    \n",
    "    # load previously trained model if appilcable\n",
    "    ckpt = tf.train.get_checkpoint_state(chk_directory)\n",
    "    if ckpt:\n",
    "        print \"loading model: \",ckpt.model_checkpoint_path\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    ActivatedUnits1 = sess.run(convolve1,feed_dict={x:np.reshape(sampleimage1,[1,64*128],order='F'),keep_prob:1.0})\n",
    "    plt.figure(1, figsize=(20,20))\n",
    "    n_columns = 3\n",
    "    n_rows = 3\n",
    "    for i in range(9):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(ActivatedUnits1[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark:\n",
    "- SETI_multi_gpu_train.py achieves ~72% accuracy after 3k epochs of data (75K steps).\n",
    "- Speed: With batch_size 128.  \n",
    "- __Notice:__ The model is not optimized to reach to its highest accuracy, you can achive better results tuning the parameters.\n",
    "\n",
    "<table border=\"1\" style=\"box-sizing: border-box; border-spacing: 30px; background-color: transparent; color: #333333; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 12px;\">\n",
    "<tbody style=\"box-sizing: border-box;\">\n",
    "<tr style=\"box-sizing: border-box;\">\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\"><span style=\"box-sizing: border-box; font-weight: bold;\">CPU Architecture</span></td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\"><span style=\"box-sizing: border-box; font-weight: bold;\">CPU cores&nbsp;</span></td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\"><span style=\"box-sizing: border-box; font-weight: bold;\">Memory&nbsp;</span></td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\"><span style=\"box-sizing: border-box; font-weight: bold;\">GPU&nbsp;</span></td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\"><span style=\"box-sizing: border-box; font-weight: bold;\">Step time (sec/batch)&nbsp;</span></td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\"><span style=\"box-sizing: border-box; font-weight: bold;\">&nbsp;Accuracy</span></td>\n",
    "</tr>\n",
    "<tr style=\"box-sizing: border-box;\">\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\">POWER8</td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:center;\">40</td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:center;\">256 GB</td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\">1 x Tesla K80</td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:center;\">~0.127 </td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\">~72% at 75K steps  (3 hours)</td>\n",
    "</tr>\n",
    "<tr style=\"box-sizing: border-box;\">\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\" >POWER8</td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:center;\">32</td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:center;\">128 GB</td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\">1 x Tesla P100 w/NVLink np8g4</td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:center;\">~0.035 </td>\n",
    "<td style=\"box-sizing: border-box; padding: 3px; text-align:left;\">~72% at 75K steps  (1 hour)</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Want to learn more?\n",
    "\n",
    "[Deep Learning with TensorFlow](http://cocl.us/SETI-NIMBIX-ML0102EN) is a free course in __cognitiveclass.ia__ where you can learn TensorFlow and Deep Learning togetherwe.\n",
    "\n",
    "Also, running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](http://cocl.us/SETI-NIMBIX-PowerAI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Authors\n",
    "\n",
    "<div class=\"teacher-image\" style=\"    float: left;\n",
    "    width: 115px;\n",
    "    height: 115px;\n",
    "    margin-right: 10px;\n",
    "    margin-bottom: 10px;\n",
    "    border: 1px solid #CCC;\n",
    "    padding: 3px;\n",
    "    border-radius: 3px;\n",
    "    text-align: center;\"><img class=\"alignnone wp-image-2258 \" src=\"https://ibm.box.com/shared/static/tyd41rlrnmfrrk78jx521eb73fljwvv0.jpg\" alt=\"Saeed Aghabozorgi\" width=\"178\" height=\"178\" /></div>\n",
    "#### Saeed Aghabozorgi\n",
    "\n",
    "[Saeed Aghabozorgi](https://ca.linkedin.com/in/saeedaghabozorgi), PhD is Sr. Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients’ ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
